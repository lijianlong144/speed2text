# AI面试助手项目任务规划

## 1. 项目概述

本项目旨在开发一个AI面试助手，通过实时语音识别技术将面试中的对话转换为文字，并且能够通过AI服务提供面试问题的回答和建议。初始阶段将专注于实现实时语音转文字功能。

### 项目目标

- 在前端通过虚拟驱动捕获Windows系统声音和麦克风输入
- 前端直接与FunASR服务建立WebSocket连接进行实时语音识别
- 用户手动选择识别文本，点击按钮将选中文本通过REST风格API发送给后端处理
- 后端通过兼容OpenAI格式的API发送请求，获取流式回答
- 通过WebSocket将流式回答实时返回给前端展示

## 2. 系统架构

### 整体架构

```
+------------------+    +------------------+    +-----------------+
| 前端应用         |    |                  |    |                 |
| (音频捕获+UI界面) |<-->| FunASR服务       |    |                 |
+--------|---------+    | (语音识别引擎)    |    |                 |
         |              +-----------------+    |                 |
         |                                     |                 |
         v                                     |                 |
+------------------+                           |                 |
| 后端服务         |                           |  AI服务         |
| (REST API)       |<------------------------->|  (流式输出)     |
+------------------+                           +-----------------+
```

### 组件说明

1. **前端应用**：
   - 负责捕获Windows系统声音和麦克风输入（使用虚拟驱动，前端应列出所有驱动供用户选择）
   - 直接与FunASR服务建立WebSocket连接进行实时语音识别
   - 允许用户选择识别文本，通过按钮点击发送到后端
   - 接收并展示流式AI回答

2. **FunASR服务**：已部署的Docker容器(ws://127.0.0.1:10096)，提供语音识别功能，与前端直接通信。

3. **后端服务**：
   - 提供REST API接收前端发送的文本请求
   - 通过兼容OpenAI格式的API发送用户文本
   - 接收流式回答并通过WebSocket转发给前端

4. **AI服务**：提供面试问题的智能回答，支持流式输出。

## 3. 技术栈选择

### 后端技术栈
- Java 17
- Spring Boot 3.x
- WebFlux (响应式编程)
- Project Reactor
- 需要添加：
  - spring-boot-starter-webflux
  - 流式处理相关库

### 前端技术栈
- HTML5 + CSS3 + JavaScript
- 可选框架：Vue.js/React.js
- WebSocket API（连接FunASR服务）
- Fetch API或Axios（与后端REST API通信）
- 需要的库：
  - Web Audio API (音频捕获与处理)
  - 用于UI组件的库

### 系统配置
- 所有配置参数（包括API密钥和提示词）应在application.yml文件中配置

## 4. 前端任务

1. **UI开发**
   - 实现左右分栏界面
   - 左侧为语音识别结果展示区
   - 右侧为AI回答展示区
   - 添加文本选择和发送功能
   - 交互按钮设计

2. **音频捕获与处理**
   - 枚举并显示所有音频设备供用户选择
   - 实现系统音频捕获功能（使用虚拟驱动）
   - 实现麦克风音频捕获功能
   - 音频数据格式转换与预处理

3. **FunASR WebSocket客户端**
   - 直接连接FunASR WebSocket服务
   - 发送音频数据
   - 接收识别结果

4. **后端REST API通信**
   - 允许用户选择识别文本
   - 通过按钮点击将选中文本通过REST API发送给后端
   - 接收流式AI回答

5. **交互逻辑**
   - 实现文本选择功能
   - 实现发送按钮功能
   - 结果展示与历史记录管理

## 5. 后端任务

1. **REST API开发**
   - 设计RESTful端点接收用户选择的文本
   - 实现控制器处理请求
   - 配置提示词模板

2. **WebSocket服务端（用于流式回答）**
   - 设置WebSocket端点
   - 处理客户端连接与消息
   - 实现流式回答转发

3. **AI服务集成**
   - 对接兼容OpenAI格式的API
   - 发送用户问题到AI服务
   - 接收AI流式回答并转发给前端

4. **反应式编程实现**
   - 使用WebFlux和Project Reactor实现非阻塞操作
   - 处理并发连接和请求
   - 实现流式响应处理

5. **配置管理**
   - 在application.yml中管理所有配置参数
   - 设置提示词模板和AI服务连接信息

## 6. 实现步骤与时间线

### 阶段1：环境搭建 (1-2天)

1. 项目初始化与依赖配置
2. 搭建基本REST API框架
3. 前端项目骨架搭建

### 阶段2：音频捕获与识别 (3-5天)

1. 实现前端音频设备枚举与选择
2. 实现音频捕获（使用虚拟驱动）
3. 实现FunASR WebSocket客户端
4. 实现实时文字显示

### 阶段3：后端服务开发 (3-4天)

1. 实现REST API端点
2. 设计并实现AI服务接口（支持流式输出）
3. 开发WebSocket服务用于流式回答
4. 配置application.yml中的提示词和参数

### 阶段4：前端界面与交互 (2-3天)

1. 完善UI界面设计
2. 实现文本选择与发送功能
3. 实现AI流式回答展示功能

### 阶段5：系统集成与测试 (2-3天)

1. 前后端集成测试
2. 性能优化
3. 问题修复与功能完善

## 7. 首要实施任务

1. **添加必要的依赖**
   - 更新pom.xml添加WebFlux和REST API相关依赖
   
2. **创建基本包结构**
   - config: 配置类和yml读取
   - controller: REST控制器
   - service: 服务层接口与实现
   - handler: WebSocket处理器（用于流式回答）
   - model: 数据模型类
   
3. **实现前端音频捕获**
   - 创建音频设备枚举组件
   - 添加设备选择下拉列表
   - 配置虚拟音频驱动连接
   - 连接FunASR服务
   
4. **实现后端REST API**
   - 创建REST控制器接收文本
   - 定义请求和响应模型
   
5. **编写前端页面原型**
   - 创建基本HTML/CSS/JS结构
   - 实现文本选择和发送功能 